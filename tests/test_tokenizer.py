"""
Tests for BPE Tokenizer.

Tests cover:
- Tokenizer training
- Encoding and decoding
- Vocabulary management
- Special tokens
- Edge cases
"""

import pytest
import tempfile
from pathlib import Path

from moellama import BPETokenizer


class TestBPETokenizer:
    """Tests for BPETokenizer."""

    def test_tokenizer_initialization(self):
        """Test tokenizer can be initialized."""
        tokenizer = BPETokenizer(vocab_size=100)
        assert tokenizer is not None
        assert tokenizer.vocab_size == 100

    def test_train_on_text(self, tiny_text):
        """Test training tokenizer on text."""
        tokenizer = BPETokenizer(vocab_size=100)
        tokenizer.train(tiny_text)

        # Check that vocab is populated
        assert len(tokenizer) > 0
        assert tokenizer.vocab_size == 100

    def test_encode_decode_roundtrip(self, sample_tokenizer, tiny_text):
        """Test that encode-decode is a valid roundtrip."""
        # Encode
        token_ids = sample_tokenizer.encode(tiny_text.strip())
        assert len(token_ids) > 0
        assert all(isinstance(tid, int) for tid in token_ids)

        # Decode
        decoded_text = sample_tokenizer.decode(token_ids)
        assert isinstance(decoded_text, str)
        # Note: Due to BPE, decoded text may not exactly match input
        assert len(decoded_text) > 0

    def test_encode_empty_string(self, sample_tokenizer):
        """Test encoding an empty string."""
        token_ids = sample_tokenizer.encode("")
        assert isinstance(token_ids, list)
        # Should return empty or minimal tokens
        assert len(token_ids) >= 0

    def test_decode_empty_list(self, sample_tokenizer):
        """Test decoding an empty token list."""
        text = sample_tokenizer.decode([])
        assert text == ""

    def test_encode_produces_valid_ids(self, sample_tokenizer, tiny_text):
        """Test that all encoded IDs are within vocab size."""
        token_ids = sample_tokenizer.encode(tiny_text)
        vocab_size = len(sample_tokenizer)

        for token_id in token_ids:
            assert 0 <= token_id < vocab_size

    def test_save_and_load_vocab(self, sample_tokenizer, temp_dir):
        """Test saving and loading vocabulary."""
        vocab_path = temp_dir / "vocab.txt"

        # Save
        sample_tokenizer.save_vocab(str(vocab_path))
        assert vocab_path.exists()

        # Load into a new tokenizer
        new_tokenizer = BPETokenizer()
        new_tokenizer.load_vocab(str(vocab_path))

        # Verify they produce same encodings
        text = "To be or not to be"
        original_ids = sample_tokenizer.encode(text)
        loaded_ids = new_tokenizer.encode(text)

        assert original_ids == loaded_ids

    def test_vocab_size_consistency(self, sample_tokenizer):
        """Test that vocab size is consistent."""
        # len() returns actual vocab size after training
        actual_vocab_size = len(sample_tokenizer)
        # For tiny text, actual size may be less than requested
        # Just verify that len() returns a positive number
        assert actual_vocab_size > 0
        assert isinstance(actual_vocab_size, int)

    def test_multiple_texts(self, sample_tokenizer):
        """Test encoding multiple different texts."""
        texts = [
            "To be or not to be",
            "That is the question",
            "Hello world",
        ]

        for text in texts:
            token_ids = sample_tokenizer.encode(text)
            assert len(token_ids) > 0
            decoded = sample_tokenizer.decode(token_ids)
            assert len(decoded) > 0


class TestSpecialCases:
    """Test special cases and edge conditions."""

    def test_very_long_text(self, sample_tokenizer):
        """Test encoding very long text."""
        long_text = "word " * 1000  # 1000 repetitions
        token_ids = sample_tokenizer.encode(long_text)
        assert len(token_ids) > 0

    def test_unicode_characters(self, sample_tokenizer):
        """Test handling of unicode characters."""
        # Simple unicode test
        text = "Hello ä¸–ç•Œ ðŸŒ"
        try:
            token_ids = sample_tokenizer.encode(text)
            assert len(token_ids) > 0
        except Exception:
            # Some tokenizers may not handle unicode well
            pytest.skip("Tokenizer does not support unicode")

    def test_repeated_encoding(self, sample_tokenizer):
        """Test that repeated encoding gives same result."""
        text = "Test encoding consistency"
        ids1 = sample_tokenizer.encode(text)
        ids2 = sample_tokenizer.encode(text)
        assert ids1 == ids2

    def test_numbers_and_punctuation(self, sample_tokenizer):
        """Test encoding numbers and punctuation."""
        text = "123 456, 789. !@#$%"
        token_ids = sample_tokenizer.encode(text)
        assert len(token_ids) > 0


class TestVocabularyManagement:
    """Test vocabulary-related functionality."""

    def test_get_vocab_size(self, sample_tokenizer):
        """Test getting vocabulary size."""
        size = len(sample_tokenizer)
        assert size > 0
        assert isinstance(size, int)

    def test_trained_tokenizer_has_vocab(self, sample_tokenizer):
        """Test that trained tokenizer has vocabulary."""
        assert len(sample_tokenizer) > 0

    def test_different_vocab_sizes(self, tiny_text):
        """Test training with different vocabulary sizes."""
        sizes = [50, 100, 200]

        for size in sizes:
            tokenizer = BPETokenizer(vocab_size=size)
            tokenizer.train(tiny_text)
            assert tokenizer.vocab_size == size
