{
  # Large-Scale Multi-Dataset Configuration with Streaming
  # Train on 100B+ tokens using streaming datasets

  # Model architecture - larger model for large-scale training
  model {
    name = "moellama_large"  # Model name (used in reports and checkpoints)
    vocab_size = 10000
    dim = 512             # Larger dimension
    num_layers = 8        # Deeper model
    num_heads = 16
    num_experts = 16      # More experts
    top_k = 2
    max_seq_len = 512     # Longer context
    dropout = 0.1
    shared_expert = true
    load_balancing_loss_coef = 0.01
  }

  # Device configuration
  device {
    type = "auto"
    num_cpu_threads = -1
    num_cpu_interop_threads = 2
    gpu_ids = [0]
    use_mps = false
  }

  # Training configuration with large streaming datasets
  training {
    # Multi-dataset mixture with streaming
    dataset_mixture = [
      {
        name = "karpathy/fineweb-edu-100b-shuffle"
        ratio = 0.45                # 45% FineWeb
        split = "train"
        streaming = true            # MUST stream for 100B tokens
        percentage = 0.10           # Use 10% = 10B tokens
      }
      {
        name = "Salesforce/wikitext"
        subset = "wikitext-103-v1"  # Larger wikitext
        ratio = 0.35                # 35% WikiText
        split = "train"
        streaming = false           # Can fit in memory
        percentage = 0.60           # Use 60%
      }
      {
        name = "m-a-p/FineFineWeb"
        ratio = 0.20                # 20% FineFineWeb
        split = "train"
        streaming = true            # Large dataset
        percentage = 0.05           # Use 5%
        domains = [                 # Filter by domains
          "aerospace"
          "biology"
          "chemistry"
          "mathematics"
          "physics"
        ]
      }
    ]

    # Mixing strategy - stop when smallest dataset exhausted
    stopping_strategy = "first_exhausted"

    # Training hyperparameters for large-scale
    batch_size = 32          # Larger batches
    learning_rate = 3e-4
    epochs = 1               # Single pass over large data
    eval_steps = 500         # Less frequent eval
    max_eval_batches = 100   # Limit eval batches for streaming
    seq_len = 512
    data_dir = "dataset"
    num_workers = 8          # More workers for data loading

    # Benchmark settings during training
    run_benchmarks = true  # Run benchmarks at end of each epoch
    benchmark_samples = 100  # Samples per benchmark during training (quick eval)

    # Logging configuration
    use_wandb = false  # Enable WandB logging (TensorBoard always enabled)
    wandb_project = "moellama-training"  # WandB project name
    wandb_run_name = null  # WandB run name (null = auto-generated)
  }

  # Inference configuration
  inference {
    max_new_tokens = 300
    temperature = 0.8
    top_k = 50
    top_p = 0.95
  }

  # Paths configuration
  paths {
    model_path = "./trained_models/large_model"
    output_dir = "./model/large_model"
  }

  # Benchmarks configuration (for standalone benchmark runner)
  benchmarks {
    # Standard LLM benchmarks to run
    enabled_benchmarks = [
      "ARC-Easy",        # AI2 Reasoning Challenge (Easy)
      "ARC-Challenge",   # AI2 Reasoning Challenge (Challenge)
      "MMLU",            # Massive Multitask Language Understanding
      "GSM8K",           # Grade School Math 8K
      "HellaSwag",       # Commonsense reasoning
      "WinoGrande"       # Pronoun resolution
    ]

    # Evaluation parameters
    max_samples = null  # Max samples per benchmark (null = all samples)
    max_new_tokens = 256  # Max tokens for generation
    temperature = 0.0  # Temperature (0.0 = greedy decoding for benchmarks)

    # Output directory for benchmark reports
    output_dir = "./report"
  }

  # Evaluation configuration
  evaluation {
    enabled = true
    enabled_benchmarks = [
      "perplexity",
      "accuracy",
      "generation"
    ]
    batch_size = 32
    test_prompts = [
      "The fundamental principles of quantum mechanics",
      "In the field of aerospace engineering",
      "Chemical reactions involving"
    ]
    generation_max_tokens = 100
    generation_temperature = 0.7
    report_path = "report_large.md"
  }
}
