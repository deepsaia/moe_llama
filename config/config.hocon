{
  # Model architecture configuration
  model {
    name = "moellama"  # Model name (used in reports and checkpoints)
    vocab_size = 10000  # Will be updated after tokenizer is created
    dim = 256         # Model dimension
    num_layers = 4    # Number of transformer layers
    num_heads = 8     # Number of attention heads
    num_experts = 8   # Number of experts in MoE layer
    top_k = 2         # Number of experts to select per token
    max_seq_len = 256 # Maximum sequence length
    dropout = 0.1     # Dropout rate
    shared_expert = true  # Include a shared expert
    load_balancing_loss_coef = 0.01  # Coefficient for load balancing loss
  }
  
  # Device configuration
  device {
    type = "auto"  # "auto", "cpu", "cuda", "mps"
    num_cpu_threads = -1  # Number of CPU threads to use
    num_cpu_interop_threads = 2  # Number of CPU interop threads
    gpu_ids = [0]  # GPUs to use (for DataParallel)
    use_mps = false  # Whether to use Apple's Metal Performance Shaders (for Macs)
  }
  
  # Training configuration
  training {
    batch_size = 16
    learning_rate = 3e-4
    epochs = 3
    eval_steps = 100
    dataset = "tiny_shakespeare"  # "tiny_shakespeare" or any HuggingFace dataset
    seq_len = 256
    data_dir = "dataset"  # Directory to store downloaded datasets
    num_workers = 4  # Number of data loading workers
    max_eval_batches = 100  # Limit eval batches for streaming datasets (null = unlimited)

    # Benchmark settings during training
    run_benchmarks = true  # Run benchmarks at end of each epoch
    benchmark_samples = 100  # Samples per benchmark during training (quick eval)

    # Logging configuration
    use_wandb = false  # Enable WandB logging (TensorBoard always enabled)
    wandb_project = "moellama-training"  # WandB project name
    wandb_run_name = null  # WandB run name (null = auto-generated)
  }
  
  # Inference configuration
  inference {
    max_new_tokens = 200
    temperature = 0.8
    top_k = 50
    top_p = 0.95
  }
  
  # Paths configuration
  paths {
    model_path = "./trained_models"  # Directory for trained model checkpoints
    output_dir = "./model"           # Temporary output directory during training
  }

  # Benchmarks configuration (for standalone benchmark runner)
  benchmarks {
    # Standard LLM benchmarks to run
    enabled_benchmarks = [
      "ARC-Easy",        # AI2 Reasoning Challenge (Easy)
      "ARC-Challenge",   # AI2 Reasoning Challenge (Challenge)
      "MMLU",            # Massive Multitask Language Understanding
      "GSM8K",           # Grade School Math 8K
      "HellaSwag",       # Commonsense reasoning
      "WinoGrande"       # Pronoun resolution
    ]

    # Evaluation parameters
    max_samples = null  # Max samples per benchmark (null = all samples)
    max_new_tokens = 256  # Max tokens for generation
    temperature = 0.0  # Temperature (0.0 = greedy decoding for benchmarks)

    # Output directory for benchmark reports
    output_dir = "./report"
  }

  # Evaluation configuration
  evaluation {
    enabled = true  # Set to false to skip evaluation

    # Which benchmarks to run (can enable/disable individually)
    enabled_benchmarks = [
      "perplexity",      # Model's prediction quality
      "accuracy",        # Token accuracy
      "generation",      # Sample text generation
      "counting",        # Letter counting ability
      "simple_math"      # Basic arithmetic
    ]

    # Evaluation parameters
    batch_size = 16

    # Generation evaluation settings
    test_prompts = [
      "Once upon a time",
      "The future of",
      "In a distant land"
    ]
    generation_max_tokens = 50
    generation_temperature = 0.8

    # Report output
    report_path = "report.md"
  }
}