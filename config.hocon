{
  # Model architecture configuration
  model {
    vocab_size = 100  # Will be updated after tokenizer is created
    dim = 256         # Model dimension
    num_layers = 4    # Number of transformer layers
    num_heads = 8     # Number of attention heads
    num_experts = 8   # Number of experts in MoE layer
    top_k = 2         # Number of experts to select per token
    max_seq_len = 128 # Maximum sequence length
    dropout = 0.1     # Dropout rate
    shared_expert = true  # Include a shared expert
    load_balancing_loss_coef = 0.01  # Coefficient for load balancing loss
  }
  
  # Device configuration
  device {
    type = "auto"  # "auto", "cpu", "cuda", "mps"
    num_cpu_threads = -1  # Number of CPU threads to use
    num_cpu_interop_threads = 2  # Number of CPU interop threads
    gpu_ids = [0]  # GPUs to use (for DataParallel)
    use_mps = false  # Whether to use Apple's Metal Performance Shaders (for Macs)
  }
  
  # Training configuration
  training {
    batch_size = 16
    learning_rate = 3e-4
    epochs = 3
    eval_steps = 100
    dataset = "tiny_shakespeare"  # Can be changed to other datasets
    seq_len = 128
    data_dir = "data"  # Directory to store datasets
    num_workers = 4  # Number of data loading workers
  }
  
  # Inference configuration
  inference {
    max_new_tokens = 1000
    temperature = 0.8
    top_k = 50
    top_p = 0.95
  }
  
  # Paths configuration
  paths {
    model_path = "./llama4_moe_model"
    output_dir = "./model"
  }
}